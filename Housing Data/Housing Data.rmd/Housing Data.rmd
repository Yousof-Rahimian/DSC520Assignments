---
title: "Untitled"
author: "Housing Data Week9 Assignment"
date: "10/31/2021"
output:
  word_document: default
  html_document: default
---

**Data for this assignment is focused on real estate transactions recorded from 1964 to 2016 and can be found in Housing.xlsx**

Explain any transformations or modifications you made to the data set?
Load the ‘Housing.csv’ in R studio by using the read command and modify the existing column of the data by performing some operations on it, such as Sale Price ~and square foot lot.

Create two variables; one that will contain the variables Sale Price and Square Foot of Lot (same variables used from previous assignment on simple regression) and one that will contain Sale Price and several additional predictors of your choice. Explain the basis for your additional predictor selections.
```{r}
## Fitting the linear regression model using Sale Price and Square Feet Lot as
##explanatory or independent (x) variable 
## Fit a model using Sale Price and Square Feet Lot as X variables
## create a model1 using Sale Price and Square Feet Lot variables 
model1 <- lm(Sale.Price ~ sq_ft_lot,  data = housing_df)
```
The basis of my additional predictor selections is what is the high correlation between the each variables.

Execute a summary() function on two variables defined in the previous step to compare the model results. What are the R2 and Adjusted R2 statistics? Explain what these results tell you about the overall model. Did the inclusion of the additional predictors help explain any large variations found in Sale Price?

```{r}
## Get summary of this model1 
summary(model1)
```
What are the R2 and Adjusted R2 statistics? R2 or Multiple R-squared:  0.01435 means approximately 1.4% of variation in Sale Price can be explained by our model. 
Did the inclusion of the additional predictors help explain any large variations found in Sale Price?
Yes, additional predictors help large variations found in Sale Price which increase the R2 to 21%


```{r}
##create a model2 using several X variables 
model2 <- lm(Sale.Price ~ sq_ft_lot + lon + lat, data = housing_df)
model2
```


```{r}
## Get summary of this model2 
summary(model2)
```
I’ve compared the model1 and model2 and below is the result. 
Model1: Multiple R-squared:  0.01435,	Adjusted R-squared:  0.01428
Modle2: Multiple R-squared:  0.2118,	Adjusted R-squared:  0.2115


```{r}
##Multiple regression model for model1 
Regression(Sale.Price ~ sq_ft_lot,  data = housing_df)
```
```{r}
##Standardized  regression coefficient for model1
reg_brief(Sale.Price ~ sq_ft_lot,  data = housing_df, standardized=TRUE)
```
```{r}
##Multiple regression for model2
Regression(Sale.Price ~ sq_ft_lot + lon + lat, data = housing_df)
```
```{r}
##Standardized  regression coefficient for model2
reg_brief(Sale.Price ~ sq_ft_lot +lon +lat,
          data = housing_df, standardized=TRUE)
```
##Calculated correlation between Sale Price and Square Feet Lot
```{r}
##Ask for confidence intervals for the model1 coefficients 
confint(model1, confi.level=0.95)
```
I have an estimated slope for our model and get the confidence interval of 95% (confint(model2, confi.level=0.95)
```
##Ask for confidence intervals for the model2 coefficients 
confint(model2, confi.level=0.95)
```
Perform casewise diagnostics to identify outliers and/or influential cases, storing each function's output in a dataframe assigned to a unique variable name.

##Perform casewise diagnostics to identify outliers for model2
##hit enter to see all plots 
```{r}
summary(model2)
plot(model2)
```
##rows 4648,4649, and 8377 are outliers. They have high leverage and influences.
##running this analysis again with out above 3 outliers and see the changes
##using comand 'slice' to remove outliers
## get rid of the above 3 outliers with 'slice' command 
```{r}
##slice of 4648,4649,8377 rows 
no_4648_4649_8377<- housing_df%>% slice(-c(4648,4649,8377))
```

```{r}
##making new version of model3
model3 <- lm(Sale.Price ~ sq_ft_lot + lon + lat, data = no_4648_4649_8377)
plot(model3)
```
## Get summary of this model3 
```{r}
summary(model3)
```
##compare summary to see differences of model2 and model3 P values
```{r}
summary(model2)
summary(model3)
```
##Use the appropriate function to show the sum of large residuals.
```{r}
sum(residuals(model2))
sum(residuals(model1))^2
```

##Which specific variables have large residuals 
##(only cases that evaluate as TRUE)
```{r}
resid(model2)
```

##cookies distance 
```{r}
cooks.distance(model1)
cooks.distance(model2)
cooks.distance(model3)
```

```{r}
##calculating the leverage model1
lev=hatvalues(model1)
lev
```

```{r}
##calculating the leverage model2
lev=hatvalues(model2)
lev
```

```{r}
##calculating the leverage model3
lev=hatvalues(model3)
lev
```

```{r}
##assumption check model2 
par(mfrow=c(2,2))
plot(model2)
```

```{r}
##assumption check model3
par(mfrow=c(2,2))
plot(model3)
```

```{r}
##independent assumption check for model
library("gvlma")
require(gvlma)
gvmodel1 <- gvlma(model1)
summary(gvmodel1)
```

```{r}
##independent assumption check for model2 
library("gvlma")
require(gvlma)
gvmodel2 <- gvlma(model2)
summary(gvmodel2)
```

```{r}
##independent assumption check for model3
library("gvlma")
require(gvlma)
gvmodel3 <- gvlma(model3)
summary(gvmodel3)
```
